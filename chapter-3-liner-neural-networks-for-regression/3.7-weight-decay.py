# =========================
# Imports
# =========================

import torch
from torch import nn                    # nn is PyTorch's neural-network module (not used directly here, but common)
from d2l import torch as d2l            # d2l = "Dive into Deep Learning" helper library (DataModule, Trainer, plotting board, etc.)
import matplotlib.pyplot as plt         # used to display the training curves that d2l logs

# ============================================================
# Data generation: synthetic high-dimensional linear regression
# ============================================================
# This section creates an artificial dataset where:
#   - Inputs X are random Gaussian features
#   - Labels y are generated by a *true* linear model:
#         y = Xw + b + noise
#   - The purpose is to demonstrate overfitting when d is large and n is small,
#     and how L2 regularization / weight decay helps.

class Data(d2l.DataModule):
    """
    d2l.DataModule is a convenience base class used by the D2L book.
    It typically stores hyperparameters (like num_train, batch_size),
    and provides a standard interface for creating dataloaders.

    Our synthetic dataset:
      - num_inputs = d (feature dimension)
      - num_train  = number of training examples
      - num_val    = number of validation examples
      - batch_size = batch size for minibatch SGD
    """
    def __init__(self, num_train, num_val, num_inputs, batch_size):
        # save_hyperparameters():
        #   D2L utility that automatically stores constructor args as fields on self.
        #   After this call you can access:
        #     self.num_train, self.num_val, self.num_inputs, self.batch_size
        self.save_hyperparameters()

        # Total number of examples (train + validation)
        n = num_train + num_val

        # self.X: input matrix of shape (n, num_inputs)
        # torch.randn draws from a standard normal distribution N(0, 1).
        # So each feature x_i is ~ N(0,1), independently.
        self.X = torch.randn(n, num_inputs)

        # noise: Gaussian noise added to the labels.
        # torch.randn(n,1) ~ N(0,1); multiplying by 0.01 makes std=0.01
        # So epsilon ~ N(0, 0.01^2)
        noise = torch.randn(n, 1) * 0.01

        # True (unknown to the learner) linear model parameters:
        # w is a column vector of shape (num_inputs, 1), each entry is 0.01
        # b is a scalar bias 0.05
        #
        # This matches the book's equation:
        #   y = 0.05 + Σ_{i=1}^d 0.01 x_i + ε
        #
        # In vector form:
        #   y = Xw + b + ε
        w, b = torch.ones((num_inputs, 1)) * 0.01, 0.05

        # Compute labels:
        # torch.matmul(self.X, w) yields shape (n,1)
        # Add scalar b (broadcast to (n,1))
        # Add noise (n,1)
        self.y = torch.matmul(self.X, w) + b + noise

    def get_dataloader(self, train):
        """
        D2L expects DataModule to provide get_dataloader(train=True/False).

        We keep X and y in one tensor each, and use slicing to select:
          - training portion: indices [0 : num_train)
          - validation portion: indices [num_train : end)
        """
        # slice(start, stop) returns a view of indices.
        # If train=True -> slice(0, num_train)
        # Else -> slice(num_train, None) i.e. till the end
        i = slice(0, self.num_train) if train else slice(self.num_train, None)

        # get_tensorloader is a D2L helper that builds a PyTorch DataLoader
        # from tensors, shuffling for training, no shuffle for validation.
        # Inputs are [X, y], the train flag, and the slice i selecting the split.
        return self.get_tensorloader([self.X, self.y], train, i)


# ============================================================
# L2 penalty (a.k.a. squared L2 norm / weight decay term)
# ============================================================

def l2_penalty(w):
    """
    L2 penalty term used for L2 regularization / weight decay.

    w is the weight vector/tensor. For a vector:
      ||w||_2^2 = sum_j w_j^2

    Here we return (sum of squares)/2:
      (1/2) * ||w||_2^2

    The factor 1/2 is convenient because:
      d/dw ( (1/2) * w^2 ) = w
    so gradients become clean.
    """
    return (w ** 2).sum() / 2


# ============================================================
# "From scratch" weight decay: add λ * (1/2)||w||^2 to the loss
# ============================================================

class WeightDecayScratch(d2l.LinearRegressionScratch):
    """
    d2l.LinearRegressionScratch is the D2L "from scratch" linear regression model.

    It usually contains:
      - parameters self.w and self.b as torch tensors (often requiring grad)
      - a forward pass to compute y_hat = Xw + b
      - a default loss (often mean squared error)
      - an optimizer step implemented manually in D2L style

    Here we override the loss to include L2 regularization:
      Loss_total = Loss_data + λ * (1/2)||w||^2

    lambd (λ) controls regularization strength:
      - λ = 0: no regularization
      - larger λ: weights are forced to stay smaller (less overfitting)
    """
    def __init__(self, num_inputs, lambd, lr, sigma=0.01):
        # Call parent initializer. Typically sets up w, b, lr, parameter init scale, etc.
        #
        # num_inputs: number of features d
        # lr: learning rate η
        # sigma: weight initialization standard deviation (small random init)
        super().__init__(num_inputs, lr, sigma)

        # Store hyperparameters so they are logged and accessible
        # self.lambd, self.lr, etc.
        self.save_hyperparameters()

    def loss(self, y_hat, y):
        """
        Compute the training loss for a minibatch.

        y_hat: model predictions for the minibatch
        y: true labels for the minibatch

        super().loss(y_hat, y) is the base loss (data-fitting term).
        In D2L linear regression it is usually mean squared error (MSE), e.g.:
          (1/2m) * Σ (y_hat - y)^2   or similar

        We add L2 regularization on weights:
          + λ * (1/2)||w||^2

        Note: Usually we regularize weights but NOT bias.
        """
        return (super().loss(y_hat, y) + self.lambd * l2_penalty(self.w))


# ============================================================
# Instantiate data and trainer
# ============================================================

# Create a high-dimensional dataset:
#   - training set: 20 samples (very small)
#   - validation set: 100 samples
#   - input dimension: 200 features (very large)
#   - batch size: 5
#
# This setup (d=200, n_train=20) strongly encourages overfitting without regularization.
data = Data(num_train=20, num_val=100, num_inputs=200, batch_size=5)

# D2L Trainer manages:
#   - looping over epochs
#   - iterating over dataloaders
#   - calling forward pass and loss
#   - backprop + parameter updates (depending on model class)
#   - logging training/validation metrics to model.board
trainer = d2l.Trainer(max_epochs=10)


# ============================================================
# Training helper for scratch model
# ============================================================

def train_scratch(lambd):
    """
    Train WeightDecayScratch model with a specified λ (lambd).

    We also:
      - set y-axis to log scale for easier comparison
      - print the final L2 penalty value of the learned weights
        (note: l2_penalty(w) = (1/2)||w||^2)
    """
    # Create the model with:
    #   num_inputs = 200
    #   lambd = λ regularization strength
    #   lr = 0.01 learning rate
    model = WeightDecayScratch(num_inputs=200, lambd=lambd, lr=0.01)

    # model.board is D2L's plotting / logging utility.
    # Using log scale helps when losses differ by orders of magnitude.
    model.board.yscale = 'log'

    # Train the model using D2L trainer.
    # This will train on the 20 samples and validate on the 100 validation samples.
    trainer.fit(model, data)

    # Print L2 penalty magnitude of weights after training.
    # This number being smaller generally indicates stronger regularization effect.
    print('L2 norm of w:', float(l2_penalty(model.w)))


# ============================================================
# Run scratch training: no regularization
# ============================================================

# λ = 0 means no penalty, model can pick large weights to fit training noise.
train_scratch(0)

# plt.show() displays the D2L plot that was populated during trainer.fit
plt.show()


# ============================================================
# Run scratch training: with regularization
# ============================================================

# λ = 3 means strong L2 penalty; weights are discouraged from becoming large.
train_scratch(3)

plt.show()


# ============================================================
# "Concise implementation": use PyTorch optimizer's built-in weight_decay
# ============================================================

class WeightDecay(d2l.LinearRegression):
    """
    d2l.LinearRegression is the "concise" model (uses nn.Linear internally).

    It typically has:
      - self.net: an nn.Linear layer
          self.net.weight : weight matrix (shape [out_features, in_features])
          self.net.bias   : bias vector
      - built-in forward pass and loss handling in D2L style

    We override configure_optimizers() to use torch.optim.SGD with weight_decay.
    In PyTorch, weight_decay implements L2 regularization (with SGD) by adding
    a term proportional to weights during the update.

    Important: We apply weight decay ONLY to weights, NOT to biases.
    """
    def __init__(self, wd, lr):
        # Parent init likely sets up nn.Linear and stores lr
        super().__init__(lr)
        self.save_hyperparameters()

        # wd = weight decay coefficient (conceptually λ)
        self.wd = wd

    def configure_optimizers(self):
        """
        Return a PyTorch optimizer.

        We create SGD with parameter groups:
          - Group 1: net.weight with weight_decay=self.wd
          - Group 2: net.bias with no weight_decay

        Parameter groups allow different optimization settings per parameter subset.
        """
        return torch.optim.SGD(
            [
                {'params': self.net.weight, 'weight_decay': self.wd},  # decay weights
                {'params': self.net.bias}                              # do not decay bias
            ],
            lr=self.lr                                                 # set learning rate (η)
        )


# ============================================================
# Train concise model with weight decay
# ============================================================

model = WeightDecay(wd=3, lr=0.01)

# Log-scale plotting again
model.board.yscale = 'log'

# Train with the same trainer and data
trainer.fit(model, data)

# model.get_w_b() is a D2L helper returning (w, b).
# For nn.Linear, w is usually shaped (1, num_inputs) or (num_outputs, num_inputs).
# We pass w into l2_penalty to measure (1/2)||w||^2.
print('L2 norm of w:', float(l2_penalty(model.get_w_b()[0])))

plt.show()
